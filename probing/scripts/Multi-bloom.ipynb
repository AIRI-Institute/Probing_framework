{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b77fd650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/protasov/Probing_framework\n"
     ]
    }
   ],
   "source": [
    "cd Probing_framework/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c5b4dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probing.pipeline import ProbingPipeline\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "466c352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb4feceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = ProbingPipeline(\n",
    "    metric_names = [\"f1\", \"accuracy\"],\n",
    "    encode_batch_size = 16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d336b614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_memory_per_gpu_dict(dtype, model_name):\n",
    "    \"\"\" try to generate the memory map based on what we know about the model and the available hardware \"\"\"\n",
    "\n",
    "    # figure out the memory map - the minimum per gpu required to load the model\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "\n",
    "    if model_name == \"bigscience/bloom\" and n_gpus == 8 and torch.cuda.get_device_properties(0).total_memory > 79*2**30:\n",
    "        # hand crafted optimized memory map for 8x80 setup over BLOOM\n",
    "        # this works with bs=40\n",
    "        return {0: '0GIB', 1: '51GIB', 2: '51GIB', 3: '51GIB', 4: '51GIB', 5: '51GIB', 6: '51GIB', 7: '51GIB'}\n",
    "\n",
    "    try:\n",
    "        # model_params calculation, as we don't have a model yet to do:\n",
    "        #model_params = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters()).values())\n",
    "\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        h = config.n_embed\n",
    "        l = config.n_layer\n",
    "        v = config.vocab_size\n",
    "        # from https://github.com/bigscience-workshop/bigscience/tree/6917a3b5fefcf439d3485ca184b4d9f6ab605150/math#model-sizing\n",
    "        model_params = l*(12*h**2 + 13*h) + v*h + 4*h\n",
    "    except:\n",
    "        print(f\"The model {model_name} has a broken config file. Please notify the owner\")\n",
    "        raise\n",
    "\n",
    "    bytes = torch.finfo(dtype).bits / 8\n",
    "    param_memory_total_in_bytes = model_params * bytes\n",
    "    # add 5% since weight sizes aren't the same and some GPU may need more memory\n",
    "    param_memory_per_gpu_in_bytes = int(param_memory_total_in_bytes / n_gpus * 1.05)\n",
    "    print(f\"Estimating {param_memory_per_gpu_in_bytes/2**30:0.2f}GB per gpu for weights\")\n",
    "\n",
    "    # check the real available memory\n",
    "    # load cuda kernels first and only measure the real free memory after loading (shorter by ~2GB)\n",
    "    torch.ones(1).cuda()\n",
    "    max_memory_per_gpu_in_bytes = torch.cuda.mem_get_info(0)[0]\n",
    "    if max_memory_per_gpu_in_bytes < param_memory_per_gpu_in_bytes:\n",
    "        raise ValueError(f\"Unable to generate the memory map automatically as the needed estimated memory per gpu ({param_memory_per_gpu_in_bytes/2**30:0.2f}GB) is bigger than the available per gpu memory ({max_memory_per_gpu_in_bytes/2**30:0.2f}GB)\")\n",
    "\n",
    "    return {i: param_memory_per_gpu_in_bytes for i in range(torch.cuda.device_count())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c77b37d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bigscience/bloom\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "experiment.transformer_model.config = AutoConfig.from_pretrained(\n",
    "            model_name, output_hidden_states=True, \n",
    "            output_attentions=True\n",
    "            )\n",
    "\n",
    "experiment.transformer_model.model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    config = experiment.transformer_model.config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=dtype,\n",
    "    max_memory = get_max_memory_per_gpu_dict(dtype, model_name)\n",
    ")\n",
    "\n",
    "experiment.transformer_model.tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, config = experiment.transformer_model.config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92d56c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Task in progress: lzh_kyoto_AdvType\n",
      "Path to data: /home/jovyan/UD/UD_Classical_Chinese-Kyoto/lzh_kyoto_AdvType.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fdc046332fd4f4c9047f26f051dcb3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Data encoding:   0%|          | 0/246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7121752ec5a7411ca798d0fdcb6b144b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Data encoding:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869d1e6193eb4359a4101b7181846580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Data encoding:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3838b127f4b437f8d16d121dc035507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Probing by layers:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiments were saved in the folder: /home/jovyan/protasov/Probing_framework/results/experiment_2022_08_12-12:22:15_PM/2022_08_12-12:50:33_PM_lzh_kyoto_AdvType\n"
     ]
    }
   ],
   "source": [
    "f = '/home/jovyan/UD/UD_Classical_Chinese-Kyoto/lzh_kyoto_AdvType.csv'\n",
    "task_name = Path(f).stem\n",
    "\n",
    "d = experiment.run(probe_task = task_name, path_to_task_file = f, verbose=True, train_epochs=20, is_scheduler = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
